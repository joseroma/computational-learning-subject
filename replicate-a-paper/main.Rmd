---
title: "Paper's replication"
author: "José Rodríguez Maldonado"
date: "Febrero de 2019"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    theme: simplex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


I have decided to choose a paper from the ones that we have in the virtual campus. In particular, I have decided to choose **Improvement of breast cancer relapse prediction in high risk intervals using artificial neural networks**. The idea is to redo the experiment using a different data set and check the matches between my results and the ones that we can find in the paper.

# Summary

First of all, it's important to sum up the paper to contextualize the project.

The objective of the study is to compare the predictive accuracy of a neural network (NN) model versus the
standard Cox proportional hazard model. The dataset used was the ‘El Álamo’ Project. The best prognostic model
generated by the NN contains as covariates age, tumour size, lymph node status, tumour grade and type of
treatment. These same variables were considered as having prognostic significance within the Cox model analysis.


Nevertheless, the predictions made by the NN were statistically significant more accurate than those from the
Cox model (p<0.0001). Seven different time intervals were also analyzed to find that the NN predictions were
much more accurate than those from the Cox model in particular in the early intervals between 1–10 and
11–20 months, and in the later one considered from 61 months to maximum follow-up time (MFT). 

Interestingly, these intervals contain regions of high relapse risk that have been observed in different studies and that are also present in the analyzed dataset.

# Method

The idea is to replicate the experiment of the paper summarized before using the TCGA dataset. And check out the differences that we can find. In order to replicate the papers experiment, I am going to follow the next steps:

- Get the TCGA survival data and replicate it in different intervals
- Choose a Feed	Forward	Neural	Network architecture by selecting hyperparameters
- Check which variables generate the best prognostic model
- Train	a	Feed	Forward	Neural	Network	to	make	survival	predictions
- Compare the results between nnet over different intervals
- Conclusion: Compare the results with the ones obtained in the paper

##Get TCGA survival data amd replicate it in different intervals

We are going to use the TCGAretriever library to get the TCGA breast cancer data.

```{r eval=FALSE}
library(TCGAretriever)
cdat.tcga<-get_clinical_data("brca_tcga_all")
```

We select the variables that we want for this experiment.

```{r eval=FALSE}
cdat.tcga.select<-subset(cdat.tcga, select = c(CASE_ID, AGE, AJCC_PATHOLOGIC_TUMOR_STAGE,AJCC_TUMOR_PATHOLOGIC_PT , MENOPAUSE_STATUS, LYMPH_NODES_EXAMINED_HE_COUNT, LYMPH_NODES_EXAMINED_IHC_COUNT,  IHC_HER2, ER_STATUS_BY_IHC, PR_STATUS_BY_IHC, OS_MONTHS, OS_STATUS))
```

Then, we truncate the patients who has lost data.

```{r eval=FALSE}
df<-cdat.tcga.select
# Quitamos valores de tiempo negativos
df<-df[df$OS_MONTHS>=0,]
df$OS_MONTHS<-as.numeric(df$OS_MONTHS)
df$OS_STATUS<-as.numeric(as.factor(df$OS_STATUS))

df<-df[!(df$IHC_HER2 %in% c("Indeterminate","Equivocal","")),]
df<-df[!(df$ER_STATUS_BY_IHC %in% c("Indeterminate","Equivocal","")),]
df<-df[!(df$PR_STATUS_BY_IHC %in% c("Indeterminate","Equivocal","")),]

# Ahora pasamos a procesar el resto de columnas que hemos seleccionado
 comprueba.fenotipo<-function(data, i){
   vect<-c()
   if(data$IHC_HER2[i]=="Positive"){
     vect<-c(vect, "HER2")
   }else if(data$IHC_HER2[i]=="Negative" && data$ER_STATUS_BY_IHC[i]=="Negative" && data$PR_STATUS_BY_IHC[i]=="Negative"){
     vect<-c(vect, "3Negative")
   }else{
     vect<-c(vect, "luminal")
   }
   return(vect)
 }
 
vect<-unlist(lapply(1:length(df$CASE_ID),function(i) comprueba.fenotipo(data=df,i)))
data.inc.phen<-cbind(df,Pheno=vect)

data.inc.phen$AGE<-as.numeric(data.inc.phen$AGE)
data.inc.phen$LYMPH_NODES_EXAMINED_HE_COUNT<-as.numeric(data.inc.phen$LYMPH_NODES_EXAMINED_HE_COUNT)

data.inc.phen<-data.inc.phen[!(data.inc.phen$LYMPH_NODES_EXAMINED_HE_COUNT %in% c("")),]
data.inc.phen$LYMPH_NODES_EXAMINED_HE_COUNT<-as.numeric(data.inc.phen$LYMPH_NODES_EXAMINED_HE_COUNT)

data.inc.phen$MENOPAUSE_STATUS<-factor(data.inc.phen$MENOPAUSE_STATUS)
levels(data.inc.phen$MENOPAUSE_STATUS) <- c('1', '2', '3','Post' , 'Pre')
data.inc.phen<-data.inc.phen[!(data.inc.phen$MENOPAUSE_STATUS %in% c("1","2","3")),]

# Renombramos columnas del data.frame
names(data.inc.phen)<-c('ID', 'age', 'tumorState', 'pathologicPT', 'menostat', 'nodesHE', 'nodesIHC', 'HER2','ER','PR', 'time','cens','Pheno')
#data.truc <- data.inc.phen [ , c(1:8,11,9,10)]
data.trunc<-data.inc.phen
```

Check the dimensions of the result dataset.

```{r echo=FALSE}
load(file=".savedData/datatrunc.RData")
```

```{r}
dim(data.trunc)
```


We are going to replicate the dataset for 5 different time intervals. 


```{r}
cat("El valor de tiempo mas pequeño es: ", min(data.trunc$time), "\nEl valor de tiempo mas grande es: ", max(data.trunc$time))
```

We are going to set the time intervals according to the maximun and minimun time.
```{r eval=FALSE}
replicate.cum <- function(step, data, min.time, max.time){
  data.cum <- data.frame()
  i<-1
  for( i in 1:length(data[,1])){
    linea <- data[i,]
    for( j in seq(min.time, max.time, by=step)){
      lin <- linea
      if(linea$time>j){
        if(linea$cens==2){
          lin$time <- j
          lin$cens <- 1
        }else{
          lin$time <- j
          lin$cens <- 2
        }
        
      }else{
        lin$time <- j
        lin$cens <- 2
      }
      data.cum <- rbind(data.cum, lin)
    }
  }
  return(data.cum)
}

interval.replication <- function(min,max, n.chunks, step, data){
  df.part <- c()
  max.time <- round((160.78/5),0)
  time.step <- round((160.78/5),0)
  min.time <- min
  for (i in 1:n.chunks) {
    print(i)
    df.part[[i]] <-  replicate.cum(step,data, min.time, max.time)
    min.time <- min.time + time.step
    max.time <- max.time + time.step
  }
  return(df.part)
}

df.replicated <-  interval.replication(min(data.trunc$time), max(data.trunc$time), 5 , 5 ,data.trunc)



```

Here we check the different datasets that we get for the data replication.

```{r echo=FALSE}
#save(df.replicated, file=".savedData/difReplications.RData")
load(file=".savedData/difReplications.RData")
```


```{r}
cat("AS we can see the datasets have the same length: \nDataFrame 1: ", dim(df.replicated[[1]]),
     "\nDataFrame 2: ", dim(df.replicated[[2]]), "\nDataFrame 3: ", dim(df.replicated[[3]]),
     "\nDataFrame 4: ", dim(df.replicated[[4]]),  "\nDataFrame 5: ", dim(df.replicated[[5]]))


cat("The mean times for the datasets are: \nDataFrame 1 (mean of time): ", mean(df.replicated[[1]]$time),
     "\nDataFrame 2 (mean of time): ", mean(df.replicated[[2]]$time), "\nDataFrame 3 (mean of time): ", mean(df.replicated[[3]]$time),
     "\nDataFrame 4 (mean of time): ", mean(df.replicated[[4]]$time),  "\nDataFrame 5 (mean of time): ", mean(df.replicated[[5]]$time))
```



## Choose a Feed	Forward	Neural	Network architecture

First we have to split a dataset into train and test.

```{r message=FALSE, warning=FALSE}
library(dplyr)
cum.train <- sample_frac(tbl = df.replicated[[2]], replace = FALSE, size = 0.9)
cum.test <- anti_join(df.replicated[[2]], cum.train)
```

For the neural network architecture we are going to check different parameters:

- Size: number of units in the hidden layer.
- Decay: parameter for weight decay.

In order to choose the best architecture, we will set all the parameters to static values and change just 1 at a time. We will start with the size, then the decay and finally the rang.

###Choose a hidden layer architecture

In class we have been told to use the `nnet package`. This package is used to create Feed-forward neural networks with a single hidden layer. So the question that we need to answer in this part turned up to be much easyer. 

**How many neurons do we want in our hidden layer?**

```{r eval=FALSE}
set.seed(7)
library(nnet)
library(caret)
library(pROC)

num.neurons <- c(1,2,3,5,6,7,8,9,10,11,12,14,15,16,18,20)

test.10.k.params <- function(formula, datos,test, num.neurons){
  auc.result<-c()
  auc.train.result<-c()
  folds <- createFolds(datos$time, k=10)
  restraining <- list();
  restest <- list();
  for(i in 1:10){
    testData <- datos[unlist(folds[i]), ]
    trainData <- datos[-unlist(folds[i]), ]
    clust.model<-nnet(formula,size=num.neurons,data=trainData, softmax = TRUE, entropy = TRUE, MaxNWts = 20000)
    predict.model.clust<-predict(clust.model, test, type = "class")
    predict.model.clust.train<-predict(clust.model, testData, type = "class")
    roc.curve.train <-roc(testData$Y[,"1"], as.numeric(predict.model.clust.train), smooth=FALSE, auc=TRUE)
    roc.curve <-roc(test$Y[,"1"], as.numeric(predict.model.clust), smooth=FALSE, auc=TRUE)
    auc.result <-c(auc.result,roc.curve$auc)
    auc.train.result <- c(auc.train.result, roc.curve.train$auc)
  }
  return(list(train = auc.train.result, test=auc.result))
}

data.cum.params<-cum.train
data.cum.params$Y <- class.ind(cum.train$cens)
data.cum.params$cens <- NULL
data.cum.params.test<-cum.test
data.cum.params.test$Y <- class.ind(cum.test$cens)
data.cum.params.test$cens <- NULL
#Con esto que esta´calculamos lo bien que predice.

means.val.size <- c()
means.val.size.train <-c()
for(i in 1:length(num.neurons)){
  tot <-test.10.k.params(formula = Y~., data.cum.params,data.cum.params.test , num.neurons = num.neurons[i])
  means.val.size <- c(means.val.size,mean(tot$test))
  means.val.size.train <- c(means.val.size.train, mean(tot$train))
}


means.val.size <- cbind(means.val.size.train, means.val.size, num.neurons)



```

We get that:

```{r echo=FALSE}
load(file=".savedData/sizennet.RData")
library(kableExtra)
```


```{r}

kable(means.val.size) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```


According to the results obtained in the 10 k-fold validation, the most appropiate size for our network seems to be about 16, where we get really good AUC. From now on I will keep the size parammeter to 16.

###Choose weight decay to avoid overfitting


```{r eval=FALSE}
set.seed(7)

decay.list <- c(0.0001,0.001,0.01,0.1,1,10)


test.10.k.params.decay <- function(formula, datos,test, decay.list){
  auc.result<-c()
  auc.train.result<-c()
  folds <- createFolds(datos$time, k=10)
  restraining <- list();
  restest <- list();
  for(i in 1:10){
    testData <- datos[unlist(folds[i]), ]
    trainData <- datos[-unlist(folds[i]), ]
    clust.model<-nnet(formula,size=16,data=trainData, softmax = TRUE, entropy = TRUE, decay = decay.list, MaxNWts = 20000)
    predict.model.clust<-predict(clust.model,test , type = "class")
     predict.model.clust.train<-predict(clust.model, testData, type = "class")
    roc.curve.train <-roc(testData$Y[,"1"], as.numeric(predict.model.clust.train), smooth=FALSE, auc=TRUE)
    roc.curve <-roc(test$Y[,"1"], as.numeric(predict.model.clust), smooth=FALSE, auc=TRUE)
    auc.result <-c(auc.result,roc.curve$auc)
    auc.train.result <- c(auc.train.result, roc.curve.train$auc)
  }
  return(list(train = auc.train.result, test=auc.result))
}


#Con esto que esta´calculamos lo bien que predice.

means.val.decay.test <- c()
means.val.decay.train <- c()
for(i in 1:length(decay.list)){
  tot <-test.10.k.params.decay(formula = Y~., data.cum.params ,data.cum.params.test, decay.list = decay.list[i])
  means.val.decay.test <- c(means.val.decay.test,mean(tot$test))
  means.val.decay.train <- c(means.val.decay.train, mean(tot$train))
}

means.val.decay <- cbind(means.val.decay.train, means.val.decay.test, decay.list)

save(means.val.decay, file=".savedData/meansDecay.RData")

```
```{r echo = FALSE}
load(file=".savedData/meansDecay.RData")
```

```{r}

kable(means.val.decay) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```


According to the results we have. I choose a decay of 0.1.

Once we clarify the hyperparameters that we are going to use, we move on to the next part. Now, we are going to compute	survival	curves/prediction	for	the	two	groups.


##Check which variables generate the best prognostic model

I want to check which is the best formula, so I am going to use the Fselector package in order to apply feature selecction.

```{r eval=FALSE}
library(FSelector)
cum.train$cens <-as.factor(cum.train$cens)
cum.try <- cum.train
cum.try$ID <- NULL
att.scores <- random.forest.importance(cens~., cum.try)
formula <- as.simple.formula(paste(cutoff.biggest.diff(att.scores), collapse="+"), "Y")

save(formula, file=".savedData/formula.RData")

```
```{r echo=FALSE}
load(file=".savedData/formula.RData")
```
```{r}
formula
```




##Train	a	Feed	Forward	Neural	Network	to	make	survival	predictions

Having all out hyperparameters and formula setted, we can check the AUC for the predictions.

```{r eval=FALSE}
test.10.k.params <- function(formula, datos, pred.data, num.neurons, num.decay){
  validation<-c()
  test <- c()
  folds <- createFolds(datos$time, k=10)
  restraining <- list();
  restest <- list();
  for(i in 1:10){
    testData <- datos[unlist(folds[i]), ]
    trainData <- datos[-unlist(folds[i]), ]
    clust.model<-nnet(formula,size=num.neurons,data=trainData, softmax = TRUE, entropy = TRUE,decay = num.decay, MaxNWts = 20000)
    predict.model.clust<-predict(clust.model, testData, type = "class")
    predict.model.clust.test<-predict(clust.model, pred.data, type = "class")
    roc.curve.validation <-roc(testData$Y[,"1"], as.numeric(predict.model.clust), smooth=FALSE, auc=TRUE)
    roc.curve.test <-roc(pred.data$Y[,"1"], as.numeric(predict.model.clust.test), smooth=FALSE, auc=TRUE)
    validation <-c(validation, roc.curve.validation$auc)
    test <-c(test, roc.curve.test$auc)
  }
  return(list(validation, test))
}

data.cum.params<-cum.train
data.cum.params$Y <- class.ind(cum.train$cens)
data.cum.params$cens <- NULL
data.cum.params.pred <- cum.test
data.cum.params.pred$Y <- class.ind(cum.test$cens)
data.cum.params.pred$cens <- NULL
#Con esto que esta´calculamos lo bien que predic
tot <-test.10.k.params(formula = Y~., data.cum.params, data.cum.params.pred , num.neurons = 16, num.decay = 0.1)

save(tot, file=".savedData/aucnnet.RData")
```

```{r echo=FALSE}
load(file=".savedData/aucnnet.RData")
```

```{r}
cat("We have a validation AUC of: ", mean(tot[[1]]), "\nWe have a test AUC of: ", mean(tot[[2]]))

```
With some small transformations we can get our predicted survival curve.
```{r eval=FALSE}
library(survival)

data.cum.params<-df.replicated[[1]]
data.cum.params$Y <- class.ind(df.replicated[[1]]$cens)
data.cum.params$cens <- NULL
data.cum.params.pred <- df.replicated[[2]]
data.cum.params.pred$Y <- class.ind(df.replicated[[2]]$cens)
data.cum.params.pred$cens <- NULL

clust.model<-nnet(formula,size=16,data=data.cum.params, softmax = TRUE, entropy = TRUE,decay = 0.1, MaxNWts = 20000)
predict.model.clust<-predict(clust.model, data.cum.params.pred, type = "class")
predict.model.clust[is.na(predict.model.clust)] <- "1"
dat.res <- cbind(predict.model.clust, data.cum.params.pred$time, data.cum.params.pred$ID, data.cum.params.pred$Pheno)
colnames(dat.res)<-c("cens","time", "ID","Pheno")
dat.res<-as.data.frame(dat.res)
dat.res$Pheno<-as.factor(dat.res$Pheno)
dat.res$time<-as.numeric(dat.res$time)
dat.res$cens<-as.numeric(dat.res$cens)
cum.test %>% group_by(ID) %>% slice(1)
dat.res %>% group_by(ID) %>% slice(1)
fit.pheno<-survfit(Surv(time, cens) ~ 1, data=data.trunc) 
fit.pheno.nnet.pred<-survfit(Surv(time, cens) ~ 1, data=dat.res)

splots<-list()
splots[[1]] <- ggsurvplot(fit.pheno, conf.int = TRUE, cex.axis=3, cex.lab=3.0, main="Real survival curve", pval = TRUE)
splots[[2]] <- ggsurvplot(fit.pheno.nnet.pred, conf.int = TRUE, censor= TRUE, cex.axis=3, cex.lab=3.0, main="NNET predicted curve", pval = TRUE)

save(splots, file=".savedData/predictedCurve.RData")

```
```{r echo=FALSE}
load(file=".savedData/predictedCurve.RData")
```

```{r message=FALSE, warning=FALSE}
library(survminer)
arrange_ggsurvplots(splots, print = TRUE, ncol =2, nrow = 1, risk.table.height = 1)
```

At the left, we find the "real" survival curves while on the right we have the survival curves that we have predicted with the neural network.

We don't have to panic, the survival curves are very different between them, this occurs because of two factors:

- We are taking into account just a short period of time, as we see in the `x` axis.
- The data used is replicated, which means, we have the same time for all the different patients.


##Compare the results between nnet over different intervals

Now, we are going to check the AUC on the different time intervals. For each interval we are going to create a train dataset consisting of all the interval times but the one we are looking that is going to be the test.

```{r eval=FALSE}
train1<-rbind(df.replicated[[2]],df.replicated[[3]],df.replicated[[4]],df.replicated[[5]])
train1$Y <- class.ind(train1$cens)
train1$cens <- NULL
train2<-rbind(df.replicated[[1]],df.replicated[[3]],df.replicated[[4]],df.replicated[[5]])
train2$Y <- class.ind(train2$cens)
train2$cens <- NULL
train3<-rbind(df.replicated[[1]],df.replicated[[2]],df.replicated[[4]],df.replicated[[5]])
train3$Y <- class.ind(train3$cens)
train3$cens <- NULL
train4<-rbind(df.replicated[[1]],df.replicated[[2]],df.replicated[[3]],df.replicated[[5]])
train4$Y <- class.ind(train4$cens)
train4$cens <- NULL
train5<-rbind(df.replicated[[1]],df.replicated[[2]],df.replicated[[3]],df.replicated[[4]])
train5$Y <- class.ind(train5$cens)
train5$cens <- NULL


test1<-df.replicated[[1]]
test1$Y <- class.ind(test1$cens)
test1$cens <- NULL
test2<-df.replicated[[2]]
test2$Y <- class.ind(test2$cens)
test2$cens <- NULL
test3<-df.replicated[[3]]
test3$Y <- class.ind(test3$cens)
test3$cens <- NULL
test4<-df.replicated[[4]]
test4$Y <- class.ind(test4$cens)
test4$cens <- NULL
test5<-df.replicated[[5]]
test5$Y <- class.ind(test5$cens)
test5$cens <- NULL

```
```{r eval=FALSE}
tot1 <-test.10.k.params(formula = Y~., train1, test1 , num.neurons = 16, num.decay = 0.1)
tot2 <-test.10.k.params(formula = Y~., train2, test2 , num.neurons = 16, num.decay = 0.1)
tot3 <-test.10.k.params(formula = Y~., train3, test3 , num.neurons = 16, num.decay = 0.1)
tot4 <-test.10.k.params(formula = Y~., train4, test4 , num.neurons = 16, num.decay = 0.1)
tot5 <-test.10.k.params(formula = Y~., train5, test5 , num.neurons = 16, num.decay = 0.1)

```

```{r echo=FALSE}
#save(tot1,tot2,tot3,tot4,tot5,file = ".savedData/AUCpred.RData")
load(file = ".savedData/AUCpred.RData")
```

```{r}
cat("First interval:\nWe have a validation AUC of: ", mean(tot1[[1]]), "\nWe have a test AUC of: ", mean(tot1[[2]]))
cat("\nSecond interval:\nWe have a validation AUC of: ", mean(tot2[[1]]), "\nWe have a test AUC of: ", mean(tot2[[2]]))
cat("\nThird interval:\nWe have a validation AUC of: ", mean(tot3[[1]]), "\nWe have a test AUC of: ", mean(tot3[[2]]))
cat("\nFourth interval:\nWe have a validation AUC of: ", mean(tot4[[1]]), "\nWe have a test AUC of: ", mean(tot4[[2]]))
cat("\nFifth interval:\nWe have a validation AUC of: ", mean(tot5[[1]]), "\nWe have a test AUC of: ", mean(tot5[[2]]))

```


##Conclusion

As we have saw in the first part, the expected results should have performed a better AUC for the time intervals containing regions of high relapse risk. That means that we are supposed to have better results for the first and last intervals of time than for the rest ones.

In the results, we have had some problems during the experimentation. In the final results we can see that we have some overfitting. This have occured due to the change of the data from the hyperparameter selection and the final ones. I have used just one interval of time because it takes less time to train the model. The way I should have trained the model is using the big dataset instead of the interval.

If we check the results, we can see that as expected, for the last and the sencond intervals we have a better AUC than for the other ones. These results match with the ones from the paper, but I wont say that it is significative. Its hard to compare this results with the ones that we have on the paper. First of all, at the first and the fourth time interval we have some overfitting that is adding a lot of noise. And also because I didn't have time to perform the cox model, I can not tell if the results are better or worse that the ones we have in the cox model.

For me its a pitty because I have spend too much time executing the different validations, and I end up having no time. The only final conclusion that I can ensure is that for the different datasets we can not choose the best hyperparameters just taking into account one of the time intervals.

